{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TDL7pAVoaXJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "BSVoIOYAKnZm",
    "outputId": "28e4cdbf-8fe7-4303-fc91-d346392c9971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bRCwnDTCkBdx",
    "outputId": "1d78d5d8-5f19-4dde-b968-c951258796d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdrive\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLAtMidUXeBq"
   },
   "source": [
    "**Ioading data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iToWbVXN74HH",
    "outputId": "38bb4f07-9d1b-4577-a426-9242ae1395ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdrive\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wDS6hA8H77zF",
    "outputId": "04cac1eb-814d-43ad-ab88-5e7de71469b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-04 14:41:29--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
      "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248100043 (237M) [application/zip]\n",
      "Saving to: ‘tiny-imagenet-200.zip’\n",
      "\n",
      "tiny-imagenet-200.z 100%[===================>] 236.61M  20.8MB/s    in 15s     \n",
      "\n",
      "2019-04-04 14:41:44 (16.3 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y475JM4r8AxC",
    "outputId": "7c1d7b99-78cc-4a4d-fd33-9435c1e7db81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdrive\tsample_data  tiny-imagenet-200\ttiny-imagenet-200.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -qq 'tiny-imagenet-200.zip'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0j64aO_dLTg2",
    "outputId": "208d5669-85f5-4930-aefb-2b293755608f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOPyz7Zx5o0n"
   },
   "outputs": [],
   "source": [
    "# load the val annotations file\n",
    "\n",
    "import os\n",
    "\n",
    "def get_annotations_map():\n",
    "\tvalAnnotationsPath = './tiny-imagenet-200/val/val_annotations.txt'\n",
    "\tvalAnnotationsFile = open(valAnnotationsPath, 'r')\n",
    "\tvalAnnotationsContents = valAnnotationsFile.read()\n",
    "\tvalAnnotations = {}\n",
    "\n",
    "\tfor line in valAnnotationsContents.splitlines():\n",
    "\t\tpieces = line.strip().split()\n",
    "\t\tvalAnnotations[pieces[0]] = pieces[1]\n",
    "\n",
    "\treturn valAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "colab_type": "code",
    "id": "XpNNOUZt6X5W",
    "outputId": "5018fb74-7d9a-46ea-9bf2-3a0272da5f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 classes\n",
      "loading training images...\n",
      "finished loading training images\n",
      "loading test images...\n",
      "finished loading test images100\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAFxCAYAAABHmx5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAFxCAYAAABHmx5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRyGot2czuo0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 4 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#Custom\n",
    "#from val_load import get_annotations_map\n",
    "\n",
    "def load_images(path,num_classes):\n",
    "    #Load images\n",
    "    \n",
    "    print('Loading ' + str(num_classes) + ' classes')\n",
    "\n",
    "    X_train=np.zeros([num_classes*500,3,64,64],dtype='uint8')\n",
    "    y_train=np.zeros([num_classes*500], dtype='uint8')\n",
    "\n",
    "    trainPath=path+'/train'\n",
    "\n",
    "    print('loading training images...');\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    annotations={}\n",
    "    for sChild in os.listdir(trainPath):\n",
    "        sChildPath = os.path.join(os.path.join(trainPath,sChild),'images')\n",
    "        annotations[sChild]=j\n",
    "        for c in os.listdir(sChildPath):\n",
    "            X=np.array(Image.open(os.path.join(sChildPath,c)))\n",
    "            if len(np.shape(X))==2:\n",
    "                X_train[i]=np.array([X,X,X])\n",
    "            else:\n",
    "                X_train[i]=np.transpose(X,(2,0,1))\n",
    "            y_train[i]=j\n",
    "            i+=1\n",
    "        j+=1\n",
    "        if (j >= num_classes):\n",
    "            break\n",
    "\n",
    "    print('finished loading training images')\n",
    "\n",
    "    val_annotations_map = get_annotations_map()\n",
    "\n",
    "    X_test = np.zeros([num_classes*50,3,64,64],dtype='uint8')\n",
    "    y_test = np.zeros([num_classes*50], dtype='uint8')\n",
    "\n",
    "\n",
    "    print('loading test images...')\n",
    "\n",
    "    i = 0\n",
    "    testPath=path+'/val/images'\n",
    "    for sChild in os.listdir(testPath):\n",
    "        if val_annotations_map[sChild] in annotations.keys():\n",
    "            sChildPath = os.path.join(testPath, sChild)\n",
    "            X=np.array(Image.open(sChildPath))\n",
    "            if len(np.shape(X))==2:\n",
    "                X_test[i]=np.array([X,X,X])\n",
    "            else:\n",
    "                X_test[i]=np.transpose(X,(2,0,1))\n",
    "            y_test[i]=annotations[val_annotations_map[sChild]]\n",
    "            i+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    print('finished loading test images'+str(i))\n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    path='./tiny-imagenet-200'\n",
    "    X_train,y_train,X_test,y_test=load_images(path,2)\n",
    "    \n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    fig1.suptitle('Train data')\n",
    "    ax1 = fig1.add_subplot(221)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.imshow(np.transpose(X_train[0],(1,2,0)))\n",
    "    print(y_train[0])\n",
    "    ax2 = fig1.add_subplot(222)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.imshow(np.transpose(X_train[499],(1,2,0)))\n",
    "    print(y_train[499])\n",
    "    ax3 = fig1.add_subplot(223)\n",
    "    ax3.axis(\"off\")\n",
    "    ax3.imshow(np.transpose(X_train[500],(1,2,0)))\n",
    "    print(y_train[500])\n",
    "    ax4 = fig1.add_subplot(224)\n",
    "    ax4.axis(\"off\")\n",
    "    ax4.imshow(np.transpose(X_train[999],(1,2,0)))\n",
    "    print(y_train[999])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    fig2.suptitle('Test data')\n",
    "    ax1 = fig2.add_subplot(221)\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.imshow(np.transpose(X_test[0],(1,2,0)))\n",
    "    print(y_test[0])\n",
    "    ax2 = fig2.add_subplot(222)\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.imshow(np.transpose(X_test[49],(1,2,0)))\n",
    "    print(y_test[49])\n",
    "    ax3 = fig2.add_subplot(223)\n",
    "    ax3.axis(\"off\")\n",
    "    ax3.imshow(np.transpose(X_test[50],(1,2,0)))\n",
    "    print(y_test[50])\n",
    "    ax4 = fig2.add_subplot(224)\n",
    "    ax4.axis(\"off\")\n",
    "    ax4.imshow(np.transpose(X_test[99],(1,2,0)))\n",
    "    print(y_test[99])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "2cquCPG07K_g",
    "outputId": "becc7a12-5c46-465e-c9cc-81158fb42c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 200 classes\n",
      "loading training images...\n",
      "finished loading training images\n",
      "loading test images...\n",
      "finished loading test images10000\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test = load_images(path,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C2FLteho8-RZ",
    "outputId": "2a05bd5a-fd5f-4ba0-ee02-bf5d06a30ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3, 64, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf57M7JjKUtw"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 64, 64,3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErWsYlgHjtL8"
   },
   "outputs": [],
   "source": [
    "# y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_oqVIGTjxsW"
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 200)\n",
    "Y_test = np_utils.to_categorical(y_test, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O55GJK6gj7gH"
   },
   "outputs": [],
   "source": [
    "# Y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79pF0UqcnN1O"
   },
   "source": [
    "\n",
    "# *Label Smoothing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb_pUk-zkUCu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def smooth_labels(y, smooth_factor):\n",
    "    '''Convert a matrix of one-hot row-vector labels into smoothed versions.\n",
    "\n",
    "    # Arguments\n",
    "        y: matrix of one-hot row-vector labels to be smoothed\n",
    "        smooth_factor: label smoothing factor (between 0 and 1)\n",
    "\n",
    "    # Returns\n",
    "        A matrix of smoothed labels.\n",
    "    '''\n",
    "    assert len(y.shape) == 2\n",
    "    if 0 <= smooth_factor <= 1:\n",
    "        # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf\n",
    "        y *= 1 - smooth_factor\n",
    "        y += smooth_factor / y.shape[1]\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Invalid label smoothing factor: ' + str(smooth_factor))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hch52JBGkRsw"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Y_train = smooth_labels(Y_train,0.5)\n",
    "# Y_test = smooth_labels(Y_test,0.5)\n",
    "# Y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AD_xRn0rYfd"
   },
   "source": [
    "#* Image augmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yErQKxVQn2HD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "w0CxkplcHBam",
    "outputId": "d611ebef-4dcc-42d1-bf55-9eff6fc3556c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val_0.JPEG</td>\n",
       "      <td>n03444034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val_1.JPEG</td>\n",
       "      <td>n04067472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val_2.JPEG</td>\n",
       "      <td>n04070727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         File      Class\n",
       "0  val_0.JPEG  n03444034\n",
       "1  val_1.JPEG  n04067472\n",
       "2  val_2.JPEG  n04070727"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
    "val_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B66nd9FKsg3a"
   },
   "outputs": [],
   "source": [
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "        \n",
    "# train_datagen = ImageDataGenerator(......)\n",
    "# train_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',\n",
    "#                                                   target_size=(256,256),\n",
    "#                                                   ......)\n",
    "# train_crops = crop_generator(train_batches, 224)\n",
    "# ......\n",
    "# net_final.fit_generator(train_crops, ......)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2_R45BPvHDZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from skimage.util import random_noise\n",
    "\n",
    "## Applying Salt and Pepper noise in images.\n",
    "\n",
    "def apply_noise(batches):\n",
    "  \n",
    "  while True:\n",
    "    batch_x, batch_y = next(batches)\n",
    "    batch_noise = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], batch_x.shape[3])) \n",
    "    \n",
    "    for i in range(batch_x.shape[0]):\n",
    "        image_array = np.asarray(batch_x[i])\n",
    "        batch_noise[i] = random_noise(image_array , mode='gaussian', var =0.01)\n",
    "    yield (batch_noise, batch_y)\n",
    "    \n",
    "# train_generator = apply_noise(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8AnRzGcj24Zh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def noising(image):\n",
    "    array = np.array(image)\n",
    "    i = random.choice(range(16,48)) # x coordinate for the top left corner of the mask\n",
    "    j = random.choice(range(16,48)) # y coordinate for the top left corner of the mask\n",
    "    array[i:i+16, j:j+16]=-1.0 # setting the pixels in the masked region to -1\n",
    "    return array\n",
    "def noise_generator(batches):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_noise = np.zeros((batch_x.shape[0], 64, 64, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_noise[i] = noising(batch_x[i])\n",
    "        yield (batch_noise, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FBFHo4D3ZN-"
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "def apply_drop(batches):\n",
    "  \n",
    "  while True:\n",
    "    batch_x, batch_y = next(batches)\n",
    "    batch_drop = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], batch_x.shape[3])) \n",
    "    \n",
    "    for i in range(batch_x.shape[0]):\n",
    "        image_array = np.asarray(batch_x[i])\n",
    "        drop = iaa.CoarseDropout((0.0, 0.5), size_percent=(0.2, 0.5))\n",
    "        batch_drop[i] = drop.augment_image(image_array)\n",
    "    yield (batch_drop, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX4C4n6TPP9P"
   },
   "outputs": [],
   "source": [
    "def blur_crop_flip_image(batches, blur_value, crop_value, flip_value):\n",
    "  seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0, blur_value)), # ex: 0.4\n",
    "    iaa.Crop(percent=(0, crop_value)), # ex: 0.2\n",
    "    iaa.Sometimes(0.3, iaa.Fliplr(flip_value))]) # 50% flip / horizontal flip of only 30% of the images passed\n",
    "  while True:\n",
    "    batch_x, batch_y = next(batches)\n",
    "    batch_augmented = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], NUM_CHANNELS)) \n",
    "    # NOTE: imgaug works on color images (3 channels). doesn't work on greyscale images with one channel\n",
    "    batch_augmented = seq.augment_images(batch_x) # calling ImgAug's augmentation on a batch of images\n",
    "    yield (batch_augmented, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMLJ1fK0aWDK"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def drop_ch(batches):\n",
    "  \n",
    "  while True:\n",
    "    batch_x, batch_y = next(batches)\n",
    "    batch_drop = np.zeros((batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], batch_x.shape[3])) \n",
    "    \n",
    "    for i in range(batch_x.shape[0]):\n",
    "        image_array = np.asarray(batch_x[i])\n",
    "        drop =iaa.CoarseDropout(0.02, size_percent=0.5, per_channel=0.5)\n",
    "        batch_drop[i] = drop.augment_image(image_array)\n",
    "    yield (batch_drop, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYcHZsISHF3i"
   },
   "outputs": [],
   "source": [
    "# Use Augmentaion parameters as required.\n",
    "import numpy as np\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    zoom_range = 0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rotation_range=25,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "train_datagen_z = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    zoom_range = (2,2),\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "# train_datagen_n = ImageDataGenerator(\n",
    "#     rescale= 1./255\n",
    "#     )\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "    \n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     zoom_range = 0.3,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     rotation_range=40,\n",
    "#     horizontal_flip=True\n",
    "#     )\n",
    "\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cCgLkthCHJHO",
    "outputId": "6ce77491-e266-466d-ccb9-edbe327f5310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 100000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "                \n",
    "train_generator_64z = train_datagen_z.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64, 64), color_mode='rgb', \n",
    "                                                         batch_size=500, class_mode='categorical', shuffle=True, seed=42)\n",
    "\n",
    "train_generator_64 = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64, 64), color_mode='rgb', \n",
    "                                                        batch_size=500, class_mode='categorical', shuffle=True, seed=42) \n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJ9367iO3tVd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVWcS06J0qo3"
   },
   "outputs": [],
   "source": [
    "tr_g = blur_crop_flip_image(train_generator_64, 0.4, 0.4, 40)\n",
    "tr = (apply_noise(train_generator_64))\n",
    "tr_cn = crop_generator(tr,44)\n",
    "tr_cn_1 = crop_generator(apply_noise(tr),40)\n",
    "tr_cn1 = crop_generator(tr,32)\n",
    "tr_m = noise_generator(train_generator_64)\n",
    "tr_d = apply_drop(train_generator_64)\n",
    "# tr_do = do(train_generator_64)\n",
    "tr_e = apply_drop(crop_generator(apply_noise(tr),32))\n",
    "\n",
    "tr_m1 = apply_drop(tr_m)\n",
    "tr_d1 = apply_drop(tr_d)\n",
    "tr_cn11 = apply_drop(tr_cn1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7u31WrW_mXUI"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "tr_ch = chain(tr,tr_cn)\n",
    "tr_ch1 = chain(tr_ch ,train_generator_64z,tr_cn,tr_cn_1,tr_cn1)\n",
    "\n",
    "tr_ch2 = chain(tr,tr_cn,tr_m)\n",
    "\n",
    "\n",
    "tr_ch3 = chain(tr,tr_cn1,tr_m,tr_d)\n",
    "\n",
    "\n",
    "tr_ch3_1 = chain(tr_m,tr_d,tr_cn1)\n",
    "tr_ch3_1g = chain(tr_m,tr_d,tr_cn1,tr_g)\n",
    "\n",
    "tr_ch3_11 = chain(tr_m,tr_d,tr_cn11)\n",
    "tr_ch3_12 = chain(tr_m,tr_m1,tr_d,tr_cn1,tr_cn11)\n",
    "tr_d_ch = apply_drop(tr_ch3_1)\n",
    "\n",
    "tr_me = chain(tr_m,tr_e)\n",
    "tr = chain(tr_ch3_12,tr_ch,tr_ch1,tr_ch2,tr_ch3,tr_ch3_11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MbWQLmWNHJKB",
    "outputId": "8a52bb0b-83bb-4673-e416-224cfec7c80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "                                                      \n",
    "validation_generator_64 = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
    "                                                    color_mode='rgb', class_mode='categorical', batch_size=500, shuffle=True, seed=42)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2tDyVijhY_F"
   },
   "source": [
    "Modelling - ***resnet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb10cXonbFZv"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import six\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution2D,SeparableConv2D\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add,concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "  \n",
    "def _conv_bn_relu(**conv_params):\n",
    "    \"\"\"Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "    \n",
    "    def f(input):\n",
    "        conv = SeparableConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                               data_format=None, dilation_rate=(1, 1), depth_multiplier=1, \n",
    "                               activation=None, use_bias=True, depthwise_initializer=kernel_initializer,\n",
    "                               pointwise_initializer=kernel_initializer, bias_initializer='zeros',\n",
    "                               depthwise_regularizer=kernel_regularizer, pointwise_regularizer=kernel_regularizer, bias_regularizer=None, activity_regularizer=None, \n",
    "                               depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)(input)\n",
    "        \n",
    "#         Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "#                       strides=strides, padding=padding,\n",
    "#                       kernel_initializer=kernel_initializer,\n",
    "#                       kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "\n",
    "    return f  \n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return SeparableConv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                               data_format=None, dilation_rate=(1, 1), depth_multiplier=1, \n",
    "                               activation=None, use_bias=True, depthwise_initializer=kernel_initializer,\n",
    "                               pointwise_initializer=kernel_initializer, bias_initializer='zeros',\n",
    "                               depthwise_regularizer=kernel_regularizer, pointwise_regularizer=kernel_regularizer,\n",
    "                               bias_regularizer=None, activity_regularizer=None, \n",
    "                               depthwise_constraint=None, pointwise_constraint=None,\n",
    "                               bias_constraint=None)(activation)\n",
    "        \n",
    "#       Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "#                       strides=strides, padding=padding,\n",
    "#                       kernel_initializer=kernel_initializer,\n",
    "#                       kernel_regularizer=kernel_regularizer)(activation)\n",
    "\n",
    "    return f\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "#     return concatenate([shortcut, residual])\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "  \n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides,\n",
    "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "\n",
    "    return f\n",
    "\n",
    "  \n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = SeparableConv2D(filters=filters, kernel_size=(3, 3), strides=init_strides, padding=\"same\",\n",
    "                                    data_format=None, dilation_rate=(1, 1), depth_multiplier=1, \n",
    "                                    activation=None, use_bias=True, depthwise_initializer=\"he_normal\",\n",
    "                                    pointwise_initializer=\"he_normal\", bias_initializer='zeros',\n",
    "                                    depthwise_regularizer=l2(1e-4), pointwise_regularizer=l2(1e-4),\n",
    "                                    bias_regularizer=None, activity_regularizer=None, \n",
    "                                    depthwise_constraint=None, pointwise_constraint=None,\n",
    "                                    bias_constraint=None)(input)\n",
    "#             Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "#                            strides=init_strides,\n",
    "#                            padding=\"same\",\n",
    "#                            kernel_initializer=\"he_normal\",\n",
    "#                            kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
    "                                  strides=init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "#         residual = keras.layers.GaussianNoise(.5)(residual)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "\n",
    "  \n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "    def f(input):\n",
    "\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
    "                              strides=init_strides,\n",
    "                              padding=\"same\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
    "                                     strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters *4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "\n",
    "    return f\n",
    "  \n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "     \n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "\n",
    "#         conv1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "        \n",
    "\n",
    "        block = conv1 #pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "\n",
    "              \n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "       # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
    "                                 strides=(1, 1))(block)\n",
    "\n",
    "        conv2 =(Convolution2D(num_outputs,(1, 1)))(pool2)\n",
    "        \n",
    "        op = conv2\n",
    "        flatten1 = Flatten()(op)#(activation=\"softmax\")\n",
    "\n",
    "        activation = Activation(\"softmax\")(flatten1)\n",
    "        \n",
    "        model = Model(inputs=input, outputs=activation)\n",
    "   \n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "      \n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2,2 ])\n",
    "      \n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNObiCDjiNQO"
   },
   "source": [
    "***Cyclic Learning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzXOu_gjlVnw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kg5JPd4ljxbS"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCSxAzCFHNiY"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "#from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "#import resnet\n",
    "\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "# csv_logger = CSVLogger('resnet50_imgnet.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoSInyTJjHAz"
   },
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# # from keras.utils.vis_utils import plot_model\n",
    "# # plot_model(model, to_file='/content/gdrive/My Drive/MyCNN/25_ld.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbM36suzGhu_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6B_AF6ng9VC"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "***64 x 64***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCh1SlN4nRUS"
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "\n",
    "# lr_finder = LRFinder(min_lr=1e-5, \n",
    "#                      max_lr=1e2, \n",
    "#                      steps_per_epoch=np.ceil(200), \n",
    "#                      epochs=3)\n",
    "\n",
    "# model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=sgd,#'adam',#'sgd',#'Adam(0.1)',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit_generator(tr_ch, epochs=4, steps_per_epoch=200, validation_steps=200,\n",
    "#                     callbacks=[lr_finder],\n",
    "#                     shuffle=True,\n",
    "#                     validation_data=validation_generator_64)\n",
    "            \n",
    "# lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4590
    },
    "colab_type": "code",
    "id": "LiM75QqQqsoB",
    "outputId": "5427d7bc-754a-421f-dc77-a4f00078da89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 64)   403         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 32, 32, 64)   4736        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 32, 32, 64)   4736        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 64)   0           activation_1[0][0]               \n",
      "                                                                 separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 32, 32, 64)   4736        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 32, 32, 64)   4736        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
      "                                                                 separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 32, 32, 64)   4736        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 32, 32, 64)   4736        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n",
      "                                                                 separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 16, 16, 128)  8896        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 16, 16, 128)  17664       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_1[0][0]                   \n",
      "                                                                 separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 16, 16, 128)  17664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 16, 16, 128)  17664       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "                                                                 separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 16, 16, 128)  17664       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 16, 16, 128)  17664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n",
      "                                                                 separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 16, 16, 128)  17664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 16, 16, 128)  17664       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 128)  0           add_6[0][0]                      \n",
      "                                                                 separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 8, 8, 256)    34176       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 256)    33024       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 8, 8, 256)    68096       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 256)    0           conv2d_2[0][0]                   \n",
      "                                                                 separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 256)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 8, 8, 256)    68096       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 256)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 8, 8, 256)    68096       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "                                                                 separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 256)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 8, 8, 256)    68096       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 256)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 8, 8, 256)    68096       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 256)    0           add_9[0][0]                      \n",
      "                                                                 separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 8, 8, 256)    68096       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 8, 8, 256)    68096       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 256)    0           add_10[0][0]                     \n",
      "                                                                 separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 256)    1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 256)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 8, 8, 256)    68096       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 8, 8, 256)    68096       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 256)    0           add_11[0][0]                     \n",
      "                                                                 separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 8, 8, 256)    68096       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 256)    1024        separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 256)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 8, 8, 256)    68096       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 256)    0           add_12[0][0]                     \n",
      "                                                                 separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 4, 4, 512)    133888      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 512)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 512)    131584      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 4, 4, 512)    267264      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 512)    0           conv2d_3[0][0]                   \n",
      "                                                                 separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 512)    2048        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 512)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 4, 4, 512)    267264      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 4, 4, 512)    267264      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 512)    0           add_14[0][0]                     \n",
      "                                                                 separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 4, 4, 512)    267264      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 512)    2048        separable_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 512)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 4, 4, 512)    267264      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 512)    0           add_15[0][0]                     \n",
      "                                                                 separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 512)    2048        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 512)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 1, 1, 200)    102600      average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 200)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,720,795\n",
      "Trainable params: 2,705,563\n",
      "Non-trainable params: 15,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0ku8-__t0Td"
   },
   "outputs": [],
   "source": [
    "schedule = SGDRScheduler(min_lr=1e-2,\n",
    "                         max_lr=1e-1,\n",
    "                         steps_per_epoch=np.ceil(200),\n",
    "                         lr_decay=0.9,\n",
    "                         cycle_length=5,\n",
    "                         mult_factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "e2pV2DQCnRUa",
    "outputId": "6dcc3770-e510-486a-9cbe-89c01d61c260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "200/200 [==============================] - 446s 2s/step - loss: 6.5095 - acc: 0.0435 - val_loss: 6.5633 - val_acc: 0.0353\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.03529, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.035.hdf5\n",
      "Epoch 2/60\n",
      "200/200 [==============================] - 422s 2s/step - loss: 5.9519 - acc: 0.0889 - val_loss: 6.3048 - val_acc: 0.0643\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.03529 to 0.06428, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.064.hdf5\n",
      "Epoch 3/60\n",
      "200/200 [==============================] - 422s 2s/step - loss: 5.5956 - acc: 0.1244 - val_loss: 5.6973 - val_acc: 0.1094\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.06428 to 0.10939, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.109.hdf5\n",
      "Epoch 4/60\n",
      "200/200 [==============================] - 423s 2s/step - loss: 5.3610 - acc: 0.1546 - val_loss: 5.3031 - val_acc: 0.1531\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.10939 to 0.15309, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.153.hdf5\n",
      "Epoch 5/60\n",
      "200/200 [==============================] - 424s 2s/step - loss: 5.2180 - acc: 0.1721 - val_loss: 5.2323 - val_acc: 0.1654\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.15309 to 0.16538, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:005-val_acc:0.165.hdf5\n",
      "Epoch 6/60\n",
      "200/200 [==============================] - 423s 2s/step - loss: 5.2431 - acc: 0.1591 - val_loss: 6.5443 - val_acc: 0.0715\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.16538\n",
      "Epoch 7/60\n",
      "200/200 [==============================] - 423s 2s/step - loss: 4.9802 - acc: 0.1855 - val_loss: 5.2337 - val_acc: 0.1571\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.16538\n",
      "Epoch 8/60\n",
      "200/200 [==============================] - 424s 2s/step - loss: 4.7458 - acc: 0.2132 - val_loss: 5.2263 - val_acc: 0.1605\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.16538\n",
      "Epoch 9/60\n",
      "200/200 [==============================] - 422s 2s/step - loss: 4.5536 - acc: 0.2373 - val_loss: 4.9077 - val_acc: 0.1880\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.16538 to 0.18805, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:009-val_acc:0.188.hdf5\n",
      "Epoch 10/60\n",
      "200/200 [==============================] - 424s 2s/step - loss: 4.3892 - acc: 0.2586 - val_loss: 4.4476 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.18805 to 0.24792, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:010-val_acc:0.248.hdf5\n",
      "Epoch 11/60\n",
      "200/200 [==============================] - 421s 2s/step - loss: 4.2508 - acc: 0.2786 - val_loss: 4.3761 - val_acc: 0.2541\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.24792 to 0.25411, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:011-val_acc:0.254.hdf5\n",
      "Epoch 12/60\n",
      "200/200 [==============================] - 423s 2s/step - loss: 4.1361 - acc: 0.2969 - val_loss: 4.4888 - val_acc: 0.2410\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.25411\n",
      "Epoch 13/60\n",
      "200/200 [==============================] - 421s 2s/step - loss: 4.0629 - acc: 0.3071 - val_loss: 4.4373 - val_acc: 0.2520\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.25411\n",
      "Epoch 14/60\n",
      "200/200 [==============================] - 424s 2s/step - loss: 4.2609 - acc: 0.2657 - val_loss: 5.7829 - val_acc: 0.1263\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.25411\n",
      "Epoch 15/60\n",
      "200/200 [==============================] - 423s 2s/step - loss: 4.1188 - acc: 0.2838 - val_loss: 5.0377 - val_acc: 0.1927\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.25411\n",
      "Epoch 16/60\n",
      "135/200 [===================>..........] - ETA: 1:49 - loss: 3.9901 - acc: 0.2989"
     ]
    }
   ],
   "source": [
    "filepath=\"/content/gdrive/My Drive/MyCNN/64/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "# model.load_weights('/content/gdrive/My Drive/MyCNN/32/epochs:028-val_acc:0.181.hdf5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(tr_ch, epochs=60, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8316
    },
    "colab_type": "code",
    "id": "pMtnMX3nt7gU",
    "outputId": "8fd0ab79-0b42-458e-bdfa-ff53dfb06030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "200/200 [==============================] - 420s 2s/step - loss: 3.5523 - acc: 0.3424 - val_loss: 5.3574 - val_acc: 0.1546\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15460, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.155.hdf5\n",
      "Epoch 2/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 3.4038 - acc: 0.3662 - val_loss: 4.1271 - val_acc: 0.2632\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.15460 to 0.26320, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.263.hdf5\n",
      "Epoch 3/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 3.2092 - acc: 0.3986 - val_loss: 3.7411 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.26320 to 0.31620, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.316.hdf5\n",
      "Epoch 4/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 3.0276 - acc: 0.4340 - val_loss: 3.7175 - val_acc: 0.3337\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.31620 to 0.33370, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.334.hdf5\n",
      "Epoch 5/60\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.8712 - acc: 0.4656 - val_loss: 3.4495 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.33370 to 0.37090, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:005-val_acc:0.371.hdf5\n",
      "Epoch 6/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.2433 - acc: 0.3866 - val_loss: 3.9953 - val_acc: 0.2908\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.37090\n",
      "Epoch 7/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 3.1696 - acc: 0.3957 - val_loss: 4.4703 - val_acc: 0.2367\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.37090\n",
      "Epoch 8/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.0612 - acc: 0.4145 - val_loss: 3.7807 - val_acc: 0.3099\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.37090\n",
      "Epoch 9/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.9386 - acc: 0.4361 - val_loss: 3.6676 - val_acc: 0.3242\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.37090\n",
      "Epoch 10/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.7995 - acc: 0.4639 - val_loss: 3.5486 - val_acc: 0.3504\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.37090\n",
      "Epoch 11/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.6681 - acc: 0.4890 - val_loss: 3.4605 - val_acc: 0.3696\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.37090\n",
      "Epoch 12/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.5468 - acc: 0.5136 - val_loss: 3.3194 - val_acc: 0.3907\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.37090 to 0.39070, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:012-val_acc:0.391.hdf5\n",
      "Epoch 13/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 2.4596 - acc: 0.5303 - val_loss: 3.3317 - val_acc: 0.3926\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.39070 to 0.39260, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:013-val_acc:0.393.hdf5\n",
      "Epoch 14/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.9217 - acc: 0.4301 - val_loss: 4.2659 - val_acc: 0.2692\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.39260\n",
      "Epoch 15/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.8854 - acc: 0.4366 - val_loss: 4.2568 - val_acc: 0.2759\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.39260\n",
      "Epoch 16/60\n",
      "200/200 [==============================] - 396s 2s/step - loss: 2.8141 - acc: 0.4485 - val_loss: 3.5370 - val_acc: 0.3414\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.39260\n",
      "Epoch 17/60\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.7361 - acc: 0.4632 - val_loss: 3.7646 - val_acc: 0.3016\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.39260\n",
      "Epoch 18/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.6507 - acc: 0.4813 - val_loss: 3.6768 - val_acc: 0.3231\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.39260\n",
      "Epoch 19/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.5547 - acc: 0.4981 - val_loss: 3.8183 - val_acc: 0.3052\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.39260\n",
      "Epoch 20/60\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.4558 - acc: 0.5181 - val_loss: 3.2220 - val_acc: 0.4015\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.39260 to 0.40150, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:020-val_acc:0.401.hdf5\n",
      "Epoch 21/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.3541 - acc: 0.5387 - val_loss: 3.1519 - val_acc: 0.4068\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.40150 to 0.40680, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:021-val_acc:0.407.hdf5\n",
      "Epoch 22/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.2399 - acc: 0.5628 - val_loss: 3.4758 - val_acc: 0.3654\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.40680\n",
      "Epoch 23/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.1379 - acc: 0.5851 - val_loss: 3.2263 - val_acc: 0.4159\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.40680 to 0.41590, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:023-val_acc:0.416.hdf5\n",
      "Epoch 24/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.0422 - acc: 0.6059 - val_loss: 3.0856 - val_acc: 0.4307\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.41590 to 0.43070, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:024-val_acc:0.431.hdf5\n",
      "Epoch 25/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 1.9873 - acc: 0.6169 - val_loss: 3.2749 - val_acc: 0.4073\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.43070\n",
      "Epoch 26/60\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.6311 - acc: 0.4732 - val_loss: 4.3294 - val_acc: 0.2734\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.43070\n",
      "Epoch 27/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.5820 - acc: 0.4844 - val_loss: 3.7507 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.43070\n",
      "Epoch 28/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.5360 - acc: 0.4957 - val_loss: 3.9845 - val_acc: 0.3099\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.43070\n",
      "Epoch 29/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.4863 - acc: 0.5067 - val_loss: 3.7384 - val_acc: 0.3324\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.43070\n",
      "Epoch 30/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.4392 - acc: 0.5161 - val_loss: 3.6828 - val_acc: 0.3323\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.43070\n",
      "Epoch 31/60\n",
      "200/200 [==============================] - 410s 2s/step - loss: 2.3827 - acc: 0.5278 - val_loss: 3.7413 - val_acc: 0.3390\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.43070\n",
      "Epoch 32/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.3071 - acc: 0.5432 - val_loss: 4.0448 - val_acc: 0.3248\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.43070\n",
      "Epoch 33/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 2.2400 - acc: 0.5569 - val_loss: 3.6102 - val_acc: 0.3555\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.43070\n",
      "Epoch 34/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.1599 - acc: 0.5747 - val_loss: 3.8971 - val_acc: 0.3307\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.43070\n",
      "Epoch 35/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.0818 - acc: 0.5911 - val_loss: 3.3837 - val_acc: 0.3897\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.43070\n",
      "Epoch 36/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.9915 - acc: 0.6114 - val_loss: 3.5107 - val_acc: 0.3893\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.43070\n",
      "Epoch 37/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 1.9064 - acc: 0.6305 - val_loss: 3.3976 - val_acc: 0.3990\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.43070\n",
      "Epoch 38/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.8118 - acc: 0.6535 - val_loss: 3.6521 - val_acc: 0.3802\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.43070\n",
      "Epoch 39/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 1.7178 - acc: 0.6751 - val_loss: 3.5337 - val_acc: 0.3875\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.43070\n",
      "Epoch 40/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.6318 - acc: 0.6947 - val_loss: 3.4778 - val_acc: 0.4100\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.43070\n",
      "Epoch 41/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.5629 - acc: 0.7094 - val_loss: 3.5426 - val_acc: 0.4024\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43070\n",
      "Epoch 42/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.4979 - acc: 0.7272 - val_loss: 3.5488 - val_acc: 0.4104\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.43070\n",
      "Epoch 43/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.4539 - acc: 0.7383 - val_loss: 3.5418 - val_acc: 0.4102\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.43070\n",
      "Epoch 44/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.3455 - acc: 0.5250 - val_loss: 5.6379 - val_acc: 0.2102\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.43070\n",
      "Epoch 45/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 2.2714 - acc: 0.5454 - val_loss: 3.9930 - val_acc: 0.3322\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.43070\n",
      "Epoch 46/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.2295 - acc: 0.5564 - val_loss: 4.3262 - val_acc: 0.3121\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.43070\n",
      "Epoch 47/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.1994 - acc: 0.5654 - val_loss: 4.4622 - val_acc: 0.2833\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.43070\n",
      "Epoch 48/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.1573 - acc: 0.5748 - val_loss: 4.0284 - val_acc: 0.3287\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.43070\n",
      "Epoch 49/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.1219 - acc: 0.5848 - val_loss: 3.8889 - val_acc: 0.3297\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.43070\n",
      "Epoch 50/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.0788 - acc: 0.5963 - val_loss: 3.6920 - val_acc: 0.3506\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.43070\n",
      "Epoch 51/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.0375 - acc: 0.6058 - val_loss: 4.0966 - val_acc: 0.3386\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.43070\n",
      "Epoch 52/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 1.9889 - acc: 0.6177 - val_loss: 4.0480 - val_acc: 0.3472\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.43070\n",
      "Epoch 53/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.9340 - acc: 0.6322 - val_loss: 4.3497 - val_acc: 0.3057\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.43070\n",
      "Epoch 54/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.8899 - acc: 0.6426 - val_loss: 4.0962 - val_acc: 0.3429\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.43070\n",
      "Epoch 55/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.8244 - acc: 0.6562 - val_loss: 4.1094 - val_acc: 0.3469\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.43070\n",
      "Epoch 56/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.7674 - acc: 0.6724 - val_loss: 3.8602 - val_acc: 0.3788\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.43070\n",
      "Epoch 57/60\n",
      "197/200 [============================>.] - ETA: 4s - loss: 1.6945 - acc: 0.6902WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "200/200 [==============================] - 420s 2s/step - loss: 3.5523 - acc: 0.3424 - val_loss: 5.3574 - val_acc: 0.1546\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15460, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.155.hdf5\n",
      "Epoch 2/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 3.4038 - acc: 0.3662 - val_loss: 4.1271 - val_acc: 0.2632\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.15460 to 0.26320, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.263.hdf5\n",
      "Epoch 3/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 3.2092 - acc: 0.3986 - val_loss: 3.7411 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.26320 to 0.31620, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.316.hdf5\n",
      "Epoch 4/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 3.0276 - acc: 0.4340 - val_loss: 3.7175 - val_acc: 0.3337\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.31620 to 0.33370, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.334.hdf5\n",
      "Epoch 5/60\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.8712 - acc: 0.4656 - val_loss: 3.4495 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.33370 to 0.37090, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:005-val_acc:0.371.hdf5\n",
      "Epoch 6/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.2433 - acc: 0.3866 - val_loss: 3.9953 - val_acc: 0.2908\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.37090\n",
      "Epoch 7/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 3.1696 - acc: 0.3957 - val_loss: 4.4703 - val_acc: 0.2367\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.37090\n",
      "Epoch 8/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.0612 - acc: 0.4145 - val_loss: 3.7807 - val_acc: 0.3099\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.37090\n",
      "Epoch 9/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.9386 - acc: 0.4361 - val_loss: 3.6676 - val_acc: 0.3242\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.37090\n",
      "Epoch 10/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.7995 - acc: 0.4639 - val_loss: 3.5486 - val_acc: 0.3504\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.37090\n",
      "Epoch 11/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.6681 - acc: 0.4890 - val_loss: 3.4605 - val_acc: 0.3696\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.37090\n",
      "Epoch 12/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.5468 - acc: 0.5136 - val_loss: 3.3194 - val_acc: 0.3907\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.37090 to 0.39070, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:012-val_acc:0.391.hdf5\n",
      "Epoch 13/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 2.4596 - acc: 0.5303 - val_loss: 3.3317 - val_acc: 0.3926\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.39070 to 0.39260, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:013-val_acc:0.393.hdf5\n",
      "Epoch 14/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.9217 - acc: 0.4301 - val_loss: 4.2659 - val_acc: 0.2692\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.39260\n",
      "Epoch 15/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.8854 - acc: 0.4366 - val_loss: 4.2568 - val_acc: 0.2759\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.39260\n",
      "Epoch 16/60\n",
      "200/200 [==============================] - 396s 2s/step - loss: 2.8141 - acc: 0.4485 - val_loss: 3.5370 - val_acc: 0.3414\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.39260\n",
      "Epoch 17/60\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.7361 - acc: 0.4632 - val_loss: 3.7646 - val_acc: 0.3016\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.39260\n",
      "Epoch 18/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.6507 - acc: 0.4813 - val_loss: 3.6768 - val_acc: 0.3231\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.39260\n",
      "Epoch 19/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.5547 - acc: 0.4981 - val_loss: 3.8183 - val_acc: 0.3052\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.39260\n",
      "Epoch 20/60\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.4558 - acc: 0.5181 - val_loss: 3.2220 - val_acc: 0.4015\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.39260 to 0.40150, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:020-val_acc:0.401.hdf5\n",
      "Epoch 21/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.3541 - acc: 0.5387 - val_loss: 3.1519 - val_acc: 0.4068\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.40150 to 0.40680, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:021-val_acc:0.407.hdf5\n",
      "Epoch 22/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.2399 - acc: 0.5628 - val_loss: 3.4758 - val_acc: 0.3654\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.40680\n",
      "Epoch 23/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.1379 - acc: 0.5851 - val_loss: 3.2263 - val_acc: 0.4159\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.40680 to 0.41590, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:023-val_acc:0.416.hdf5\n",
      "Epoch 24/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.0422 - acc: 0.6059 - val_loss: 3.0856 - val_acc: 0.4307\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.41590 to 0.43070, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:024-val_acc:0.431.hdf5\n",
      "Epoch 25/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 1.9873 - acc: 0.6169 - val_loss: 3.2749 - val_acc: 0.4073\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.43070\n",
      "Epoch 26/60\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.6311 - acc: 0.4732 - val_loss: 4.3294 - val_acc: 0.2734\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.43070\n",
      "Epoch 27/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.5820 - acc: 0.4844 - val_loss: 3.7507 - val_acc: 0.3200\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.43070\n",
      "Epoch 28/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.5360 - acc: 0.4957 - val_loss: 3.9845 - val_acc: 0.3099\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.43070\n",
      "Epoch 29/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.4863 - acc: 0.5067 - val_loss: 3.7384 - val_acc: 0.3324\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.43070\n",
      "Epoch 30/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.4392 - acc: 0.5161 - val_loss: 3.6828 - val_acc: 0.3323\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.43070\n",
      "Epoch 31/60\n",
      "200/200 [==============================] - 410s 2s/step - loss: 2.3827 - acc: 0.5278 - val_loss: 3.7413 - val_acc: 0.3390\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.43070\n",
      "Epoch 32/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.3071 - acc: 0.5432 - val_loss: 4.0448 - val_acc: 0.3248\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.43070\n",
      "Epoch 33/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 2.2400 - acc: 0.5569 - val_loss: 3.6102 - val_acc: 0.3555\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.43070\n",
      "Epoch 34/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.1599 - acc: 0.5747 - val_loss: 3.8971 - val_acc: 0.3307\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.43070\n",
      "Epoch 35/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.0818 - acc: 0.5911 - val_loss: 3.3837 - val_acc: 0.3897\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.43070\n",
      "Epoch 36/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.9915 - acc: 0.6114 - val_loss: 3.5107 - val_acc: 0.3893\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.43070\n",
      "Epoch 37/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 1.9064 - acc: 0.6305 - val_loss: 3.3976 - val_acc: 0.3990\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.43070\n",
      "Epoch 38/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.8118 - acc: 0.6535 - val_loss: 3.6521 - val_acc: 0.3802\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.43070\n",
      "Epoch 39/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 1.7178 - acc: 0.6751 - val_loss: 3.5337 - val_acc: 0.3875\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.43070\n",
      "Epoch 40/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.6318 - acc: 0.6947 - val_loss: 3.4778 - val_acc: 0.4100\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.43070\n",
      "Epoch 41/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.5629 - acc: 0.7094 - val_loss: 3.5426 - val_acc: 0.4024\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43070\n",
      "Epoch 42/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.4979 - acc: 0.7272 - val_loss: 3.5488 - val_acc: 0.4104\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.43070\n",
      "Epoch 43/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.4539 - acc: 0.7383 - val_loss: 3.5418 - val_acc: 0.4102\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.43070\n",
      "Epoch 44/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.3455 - acc: 0.5250 - val_loss: 5.6379 - val_acc: 0.2102\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.43070\n",
      "Epoch 45/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 2.2714 - acc: 0.5454 - val_loss: 3.9930 - val_acc: 0.3322\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.43070\n",
      "Epoch 46/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.2295 - acc: 0.5564 - val_loss: 4.3262 - val_acc: 0.3121\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.43070\n",
      "Epoch 47/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.1994 - acc: 0.5654 - val_loss: 4.4622 - val_acc: 0.2833\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.43070\n",
      "Epoch 48/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.1573 - acc: 0.5748 - val_loss: 4.0284 - val_acc: 0.3287\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.43070\n",
      "Epoch 49/60\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.1219 - acc: 0.5848 - val_loss: 3.8889 - val_acc: 0.3297\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.43070\n",
      "Epoch 50/60\n",
      "200/200 [==============================] - 398s 2s/step - loss: 2.0788 - acc: 0.5963 - val_loss: 3.6920 - val_acc: 0.3506\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.43070\n",
      "Epoch 51/60\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.0375 - acc: 0.6058 - val_loss: 4.0966 - val_acc: 0.3386\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.43070\n",
      "Epoch 52/60\n",
      "200/200 [==============================] - 399s 2s/step - loss: 1.9889 - acc: 0.6177 - val_loss: 4.0480 - val_acc: 0.3472\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.43070\n",
      "Epoch 53/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.9340 - acc: 0.6322 - val_loss: 4.3497 - val_acc: 0.3057\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.43070\n",
      "Epoch 54/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.8899 - acc: 0.6426 - val_loss: 4.0962 - val_acc: 0.3429\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.43070\n",
      "Epoch 55/60\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.8244 - acc: 0.6562 - val_loss: 4.1094 - val_acc: 0.3469\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.43070\n",
      "Epoch 56/60\n",
      "200/200 [==============================] - 402s 2s/step - loss: 1.7674 - acc: 0.6724 - val_loss: 3.8602 - val_acc: 0.3788\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.43070\n",
      "Epoch 57/60\n",
      "200/200 [==============================] - 408s 2s/step - loss: 1.6949 - acc: 0.6901 - val_loss: 4.5562 - val_acc: 0.3162\n",
      "200/200 [==============================] - 408s 2s/step - loss: 1.6949 - acc: 0.6901 - val_loss: 4.5562 - val_acc: 0.3162\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.43070\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.43070\n",
      "Epoch 58/60\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.6371 - acc: 0.7035 - val_loss: 4.0654 - val_acc: 0.3726\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.6371 - acc: 0.7035 - val_loss: 4.0654 - val_acc: 0.3726\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.43070\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.43070\n",
      "Epoch 59/60\n",
      "200/200 [==============================] - 406s 2s/step - loss: 1.5633 - acc: 0.7212 - val_loss: 3.9254 - val_acc: 0.3782\n",
      "200/200 [==============================] - 406s 2s/step - loss: 1.5633 - acc: 0.7212 - val_loss: 3.9254 - val_acc: 0.3782\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.43070\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.43070\n",
      "Epoch 60/60\n",
      "200/200 [==============================] - 407s 2s/step - loss: 1.4904 - acc: 0.7398 - val_loss: 3.7730 - val_acc: 0.3905\n",
      "200/200 [==============================] - 407s 2s/step - loss: 1.4904 - acc: 0.7398 - val_loss: 3.7730 - val_acc: 0.3905\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.43070\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.43070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7ed168400>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7ed168400>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"/content/gdrive/My Drive/MyCNN/64/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/MyCNN/64/epochs:025-val_acc:0.340.hdf5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(tr_ch, epochs=60, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6252
    },
    "colab_type": "code",
    "id": "2rD4oU1HxCBJ",
    "outputId": "940dab7c-f3a6-48a5-8cd6-d64383537070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 430s 2s/step - loss: 2.7677 - acc: 0.4472 - val_loss: 3.9693 - val_acc: 0.2714\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.27142, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.271.hdf5\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 401s 2s/step - loss: 2.6559 - acc: 0.4720 - val_loss: 3.8835 - val_acc: 0.3140\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.27142 to 0.31396, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.314.hdf5\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 397s 2s/step - loss: 2.4391 - acc: 0.5174 - val_loss: 3.3940 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.31396 to 0.37094, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.371.hdf5\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.2189 - acc: 0.5676 - val_loss: 3.0810 - val_acc: 0.4236\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.37094 to 0.42363, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.424.hdf5\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 404s 2s/step - loss: 2.0140 - acc: 0.6140 - val_loss: 3.1254 - val_acc: 0.4243\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.42363 to 0.42432, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:005-val_acc:0.424.hdf5\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 405s 2s/step - loss: 2.5648 - acc: 0.4883 - val_loss: 4.1418 - val_acc: 0.2948\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.42432\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.5564 - acc: 0.4928 - val_loss: 3.8964 - val_acc: 0.3043\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.42432\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 406s 2s/step - loss: 2.4523 - acc: 0.5148 - val_loss: 3.6358 - val_acc: 0.3492\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.42432\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.3234 - acc: 0.5432 - val_loss: 3.5486 - val_acc: 0.3484\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.42432\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 2.1617 - acc: 0.5775 - val_loss: 3.2421 - val_acc: 0.4066\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.42432\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.0039 - acc: 0.6141 - val_loss: 3.2940 - val_acc: 0.4047\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.42432\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 398s 2s/step - loss: 1.8464 - acc: 0.6511 - val_loss: 3.2096 - val_acc: 0.4217\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.42432\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 405s 2s/step - loss: 1.7219 - acc: 0.6812 - val_loss: 3.2784 - val_acc: 0.4264\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.42432 to 0.42638, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:013-val_acc:0.426.hdf5\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.3876 - acc: 0.5244 - val_loss: 4.1330 - val_acc: 0.3160\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.42638\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.3964 - acc: 0.5230 - val_loss: 4.0724 - val_acc: 0.3062\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.42638\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.3379 - acc: 0.5383 - val_loss: 3.8459 - val_acc: 0.3227\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.42638\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.2583 - acc: 0.5576 - val_loss: 3.8766 - val_acc: 0.3269\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.42638\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 2.1662 - acc: 0.5779 - val_loss: 3.4397 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.42638\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 2.0635 - acc: 0.5992 - val_loss: 3.6113 - val_acc: 0.3748\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.42638\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.9517 - acc: 0.6273 - val_loss: 3.4609 - val_acc: 0.3901\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.42638\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 392s 2s/step - loss: 1.8221 - acc: 0.6550 - val_loss: 3.4530 - val_acc: 0.4033\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.42638\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 389s 2s/step - loss: 1.6950 - acc: 0.6853 - val_loss: 3.6735 - val_acc: 0.3833\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.42638\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 386s 2s/step - loss: 1.5692 - acc: 0.7185 - val_loss: 3.3889 - val_acc: 0.4310\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.42638 to 0.43096, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:023-val_acc:0.431.hdf5\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 385s 2s/step - loss: 1.4662 - acc: 0.7444 - val_loss: 3.5748 - val_acc: 0.4147\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.43096\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 383s 2s/step - loss: 1.4085 - acc: 0.7581 - val_loss: 3.3520 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.43096 to 0.43749, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:025-val_acc:0.437.hdf5\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 382s 2s/step - loss: 2.2459 - acc: 0.5541 - val_loss: 4.4753 - val_acc: 0.2918\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.43749\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 379s 2s/step - loss: 2.2371 - acc: 0.5572 - val_loss: 4.3096 - val_acc: 0.2954\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.43749\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 399s 2s/step - loss: 2.1872 - acc: 0.5733 - val_loss: 4.2054 - val_acc: 0.3294\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.43749\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.1413 - acc: 0.5839 - val_loss: 3.9135 - val_acc: 0.3541\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.43749\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 402s 2s/step - loss: 2.0834 - acc: 0.5990 - val_loss: 3.7323 - val_acc: 0.3665\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.43749\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 407s 2s/step - loss: 2.0350 - acc: 0.6122 - val_loss: 3.7039 - val_acc: 0.3840\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.43749\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 414s 2s/step - loss: 1.9551 - acc: 0.6309 - val_loss: 3.8758 - val_acc: 0.3566\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.43749\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.8811 - acc: 0.6486 - val_loss: 3.8463 - val_acc: 0.3736\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.43749\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 1.7977 - acc: 0.6676 - val_loss: 3.6028 - val_acc: 0.4019\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.43749\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.6962 - acc: 0.6932 - val_loss: 3.6165 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.43749\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.6034 - acc: 0.7162 - val_loss: 3.6750 - val_acc: 0.4097\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.43749\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.4989 - acc: 0.7432 - val_loss: 3.6352 - val_acc: 0.4153\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.43749\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.4074 - acc: 0.7660 - val_loss: 3.6631 - val_acc: 0.4071\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.43749\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.3156 - acc: 0.7895 - val_loss: 3.7104 - val_acc: 0.4197\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.43749\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 1.2243 - acc: 0.8131 - val_loss: 3.7690 - val_acc: 0.4130\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.43749\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 1.1667 - acc: 0.8292 - val_loss: 3.7502 - val_acc: 0.4174\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43749\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.1073 - acc: 0.8464 - val_loss: 3.5418 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.43749 to 0.44267, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:042-val_acc:0.443.hdf5\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.0703 - acc: 0.8546 - val_loss: 3.8499 - val_acc: 0.4153\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.44267\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 2.0589 - acc: 0.6013 - val_loss: 5.1244 - val_acc: 0.2846\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.44267\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 2.0253 - acc: 0.6112 - val_loss: 4.7596 - val_acc: 0.2911\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.44267\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 1.9859 - acc: 0.6219 - val_loss: 4.0279 - val_acc: 0.3603\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.44267\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.9433 - acc: 0.6359 - val_loss: 4.7395 - val_acc: 0.2996\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.44267\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.9042 - acc: 0.6485 - val_loss: 4.5534 - val_acc: 0.3241\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.44267\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.8727 - acc: 0.6587 - val_loss: 4.4591 - val_acc: 0.3210\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.44267\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.8234 - acc: 0.6713 - val_loss: 4.1106 - val_acc: 0.3660\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.44267\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.7860 - acc: 0.6816 - val_loss: 4.2470 - val_acc: 0.3534\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.44267\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 416s 2s/step - loss: 1.7431 - acc: 0.6923 - val_loss: 4.2314 - val_acc: 0.3622\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.44267\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.6804 - acc: 0.7099 - val_loss: 4.5288 - val_acc: 0.3346\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.44267\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 410s 2s/step - loss: 1.6367 - acc: 0.7211 - val_loss: 4.4789 - val_acc: 0.3520\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.44267\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.5666 - acc: 0.7408 - val_loss: 4.1576 - val_acc: 0.3675\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.44267\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 415s 2s/step - loss: 1.5130 - acc: 0.7552 - val_loss: 4.2397 - val_acc: 0.3727\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.44267\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.4392 - acc: 0.7731 - val_loss: 4.5472 - val_acc: 0.3639\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.44267\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.3810 - acc: 0.7877 - val_loss: 4.2341 - val_acc: 0.3679\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.44267\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 1.3056 - acc: 0.8083 - val_loss: 4.2564 - val_acc: 0.3870\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.44267\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.2417 - acc: 0.8245 - val_loss: 4.1904 - val_acc: 0.3924\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.44267\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.1767 - acc: 0.8425 - val_loss: 4.1094 - val_acc: 0.4025\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.44267\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.1134 - acc: 0.8599 - val_loss: 4.2401 - val_acc: 0.3992\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.44267\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 1.0608 - acc: 0.8747 - val_loss: 4.1359 - val_acc: 0.4076\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.44267\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 414s 2s/step - loss: 1.0074 - acc: 0.8885 - val_loss: 4.3373 - val_acc: 0.4010\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.44267\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.9639 - acc: 0.9000 - val_loss: 4.2881 - val_acc: 0.4095\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.44267\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.9296 - acc: 0.9087 - val_loss: 4.1111 - val_acc: 0.4215\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.44267\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.8920 - acc: 0.9201 - val_loss: 4.3043 - val_acc: 0.4068\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.44267\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 0.8661 - acc: 0.9259 - val_loss: 4.1004 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.44267\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 0.8443 - acc: 0.9316 - val_loss: 4.1969 - val_acc: 0.4235\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.44267\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.8311 - acc: 0.9331 - val_loss: 4.3415 - val_acc: 0.4103\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.44267\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.8599 - acc: 0.6592 - val_loss: 5.1813 - val_acc: 0.3196\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.44267\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 419s 2s/step - loss: 1.8517 - acc: 0.6608 - val_loss: 4.9114 - val_acc: 0.3172\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.44267\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 412s 2s/step - loss: 1.7753 - acc: 0.6829 - val_loss: 4.7162 - val_acc: 0.3261\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.44267\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 411s 2s/step - loss: 1.7219 - acc: 0.6996 - val_loss: 4.5742 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.44267\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 413s 2s/step - loss: 1.7074 - acc: 0.7085 - val_loss: 4.4772 - val_acc: 0.3594\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.44267\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 408s 2s/step - loss: 1.6895 - acc: 0.7126 - val_loss: 4.6807 - val_acc: 0.3433\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.44267\n",
      "Epoch 77/100\n",
      " 46/200 [=====>........................] - ETA: 3:34 - loss: 1.6410 - acc: 0.7277"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-cf2a581a0228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     validation_data=validation_generator_64)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath=\"/content/gdrive/My Drive/MyCNN/64/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/MyCNN/64/epochs:024-val_acc:0.431.hdf5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(tr_ch2, epochs=100, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2145
    },
    "colab_type": "code",
    "id": "iArwMwkZbLrV",
    "outputId": "dae833fd-be3c-47eb-edbe-99a4792fb10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 397s 2s/step - loss: 2.5103 - acc: 0.5104 - val_loss: 4.6878 - val_acc: 0.2374\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.23740, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.237.hdf5\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 367s 2s/step - loss: 2.1129 - acc: 0.5966 - val_loss: 3.8267 - val_acc: 0.3656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.23740 to 0.36560, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.366.hdf5\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 366s 2s/step - loss: 1.7089 - acc: 0.6979 - val_loss: 3.5248 - val_acc: 0.4191\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.36560 to 0.41910, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.419.hdf5\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 365s 2s/step - loss: 1.3754 - acc: 0.7901 - val_loss: 3.4161 - val_acc: 0.4499\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.41910 to 0.44990, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.450.hdf5\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 367s 2s/step - loss: 2.2048 - acc: 0.5793 - val_loss: 5.0123 - val_acc: 0.2440\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.44990\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 363s 2s/step - loss: 2.0992 - acc: 0.6067 - val_loss: 4.4835 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.44990\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 365s 2s/step - loss: 1.8893 - acc: 0.6605 - val_loss: 3.9406 - val_acc: 0.3657\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.44990\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 365s 2s/step - loss: 1.6256 - acc: 0.7267 - val_loss: 3.8746 - val_acc: 0.3895\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.44990\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 365s 2s/step - loss: 1.3734 - acc: 0.7957 - val_loss: 3.6134 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.44990\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 363s 2s/step - loss: 1.2039 - acc: 0.8455 - val_loss: 3.8004 - val_acc: 0.4188\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.44990\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 362s 2s/step - loss: 2.0755 - acc: 0.6133 - val_loss: 5.2153 - val_acc: 0.2508\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.44990\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 358s 2s/step - loss: 2.0438 - acc: 0.6242 - val_loss: 5.1740 - val_acc: 0.2733\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.44990\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 360s 2s/step - loss: 1.9349 - acc: 0.6528 - val_loss: 4.2053 - val_acc: 0.3485\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.44990\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 359s 2s/step - loss: 1.7675 - acc: 0.6953 - val_loss: 4.7458 - val_acc: 0.3138\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.44990\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 371s 2s/step - loss: 1.5977 - acc: 0.7416 - val_loss: 4.1145 - val_acc: 0.3770\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.44990\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 369s 2s/step - loss: 1.4247 - acc: 0.7871 - val_loss: 3.9994 - val_acc: 0.3969\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.44990\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 374s 2s/step - loss: 1.2553 - acc: 0.8338 - val_loss: 4.0324 - val_acc: 0.3991\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.44990\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 376s 2s/step - loss: 1.1287 - acc: 0.8705 - val_loss: 3.7990 - val_acc: 0.4327\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.44990\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 361s 2s/step - loss: 1.0578 - acc: 0.8896 - val_loss: 3.9177 - val_acc: 0.4301\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.44990\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 363s 2s/step - loss: 1.9366 - acc: 0.6497 - val_loss: 5.2854 - val_acc: 0.2923\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.44990\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 372s 2s/step - loss: 1.9416 - acc: 0.6516 - val_loss: 4.4307 - val_acc: 0.3401\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.44990\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.8737 - acc: 0.6702 - val_loss: 4.5433 - val_acc: 0.3228\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.44990\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.8065 - acc: 0.6900 - val_loss: 4.7636 - val_acc: 0.3350\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.44990\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.7115 - acc: 0.7164 - val_loss: 5.3555 - val_acc: 0.2864\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.44990\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 379s 2s/step - loss: 1.6316 - acc: 0.7380 - val_loss: 4.2106 - val_acc: 0.3705\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.44990\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 370s 2s/step - loss: 1.4998 - acc: 0.7744 - val_loss: 4.1986 - val_acc: 0.3649\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.44990\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 369s 2s/step - loss: 1.3762 - acc: 0.8055 - val_loss: 4.2834 - val_acc: 0.3724\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.44990\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 366s 2s/step - loss: 1.2718 - acc: 0.8328 - val_loss: 4.3138 - val_acc: 0.4041\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.44990\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 359s 2s/step - loss: 1.1595 - acc: 0.8651 - val_loss: 4.2912 - val_acc: 0.3858\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.44990\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 355s 2s/step - loss: 1.0699 - acc: 0.8906 - val_loss: 4.1288 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.44990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43059181d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"/content/gdrive/My Drive/MyCNN/64/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "schedule = SGDRScheduler(min_lr=1e-2,\n",
    "                         max_lr=1e-1,\n",
    "                         steps_per_epoch=np.ceil(200),\n",
    "                         lr_decay=0.9,\n",
    "                         cycle_length=4,\n",
    "                         mult_factor=1.5)\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/MyCNN/64/epochs:042-val_acc:0.443.hdf5')\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(tr_ch3, epochs=30, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "EFpgfk-efyx-",
    "outputId": "21306152-f929-4106-8d10-81f797ba139e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.0625 - acc: 0.8839 - val_loss: 3.9382 - val_acc: 0.4307\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.44990\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 368s 2s/step - loss: 1.0142 - acc: 0.8969 - val_loss: 3.9218 - val_acc: 0.4327\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.44990\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 357s 2s/step - loss: 0.9698 - acc: 0.9097 - val_loss: 3.9487 - val_acc: 0.4344\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.44990\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 355s 2s/step - loss: 0.9545 - acc: 0.9115 - val_loss: 3.9593 - val_acc: 0.4340\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.44990\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 373s 2s/step - loss: 0.9481 - acc: 0.9120 - val_loss: 4.1211 - val_acc: 0.4279\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.44990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42ea195b38>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(tr_ch3, epochs=5, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "tfAK34IUpAgM",
    "outputId": "86295918-e0c9-4544-ae45-a0c0fca3029b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 338s 2s/step - loss: 1.5519 - acc: 0.7481 - val_loss: 3.9773 - val_acc: 0.4250\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.44990\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 323s 2s/step - loss: 1.4068 - acc: 0.7799 - val_loss: 4.4127 - val_acc: 0.3692\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.44990\n",
      "Epoch 3/30\n",
      "199/200 [============================>.] - ETA: 1s - loss: 1.4720 - acc: 0.7613"
     ]
    }
   ],
   "source": [
    "model.fit_generator(tr_ch3_1, epochs=30, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2074
    },
    "colab_type": "code",
    "id": "CF8n7FQcRqAl",
    "outputId": "402844a7-0fca-472a-a9d9-97ff02eda3b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 350s 2s/step - loss: 2.1683 - acc: 0.6175 - val_loss: 5.8675 - val_acc: 0.2600\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.26000, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:001-val_acc:0.260.hdf5\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 334s 2s/step - loss: 1.8135 - acc: 0.7058 - val_loss: 4.6373 - val_acc: 0.3646\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.26000 to 0.36460, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:002-val_acc:0.365.hdf5\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 1.3959 - acc: 0.8236 - val_loss: 3.4875 - val_acc: 0.4889\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.36460 to 0.48890, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:003-val_acc:0.489.hdf5\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.1326 - acc: 0.9004 - val_loss: 3.5564 - val_acc: 0.4946\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.48890 to 0.49460, saving model to /content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.495.hdf5\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 330s 2s/step - loss: 1.8741 - acc: 0.6928 - val_loss: 6.3901 - val_acc: 0.2446\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.49460\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 330s 2s/step - loss: 1.8398 - acc: 0.7024 - val_loss: 4.2707 - val_acc: 0.3816\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.49460\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 1.5980 - acc: 0.7698 - val_loss: 4.0243 - val_acc: 0.4316\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.49460\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.3297 - acc: 0.8443 - val_loss: 3.7057 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.49460\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 1.1269 - acc: 0.9024 - val_loss: 3.7767 - val_acc: 0.4683\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.49460\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 1.0233 - acc: 0.9333 - val_loss: 3.6887 - val_acc: 0.4883\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.49460\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.7430 - acc: 0.7256 - val_loss: 6.1556 - val_acc: 0.2827\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.49460\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 1.7946 - acc: 0.7162 - val_loss: 5.3133 - val_acc: 0.3184\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.49460\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 330s 2s/step - loss: 1.6536 - acc: 0.7546 - val_loss: 4.5650 - val_acc: 0.3759\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.49460\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.5026 - acc: 0.7961 - val_loss: 4.1755 - val_acc: 0.4183\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.49460\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 328s 2s/step - loss: 1.3197 - acc: 0.8478 - val_loss: 4.2183 - val_acc: 0.4356\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.49460\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 1.1752 - acc: 0.8874 - val_loss: 4.0240 - val_acc: 0.4633\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.49460\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 330s 2s/step - loss: 1.0529 - acc: 0.9220 - val_loss: 4.1278 - val_acc: 0.4555\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.49460\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 0.9744 - acc: 0.9434 - val_loss: 3.8928 - val_acc: 0.4765\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.49460\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 0.9331 - acc: 0.9542 - val_loss: 3.9282 - val_acc: 0.4810\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.49460\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.6367 - acc: 0.7490 - val_loss: 5.6105 - val_acc: 0.3172\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.49460\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 334s 2s/step - loss: 1.7135 - acc: 0.7327 - val_loss: 6.5727 - val_acc: 0.2672\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.49460\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.6344 - acc: 0.7569 - val_loss: 5.4853 - val_acc: 0.3262\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.49460\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.5612 - acc: 0.7778 - val_loss: 4.9464 - val_acc: 0.3595\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.49460\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 1.4628 - acc: 0.8057 - val_loss: 4.7458 - val_acc: 0.3663\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.49460\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.3631 - acc: 0.8311 - val_loss: 4.4185 - val_acc: 0.4219\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.49460\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 329s 2s/step - loss: 1.2607 - acc: 0.8609 - val_loss: 4.3764 - val_acc: 0.4353\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.49460\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 1.1523 - acc: 0.8903 - val_loss: 4.2920 - val_acc: 0.4295\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.49460\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 332s 2s/step - loss: 1.0594 - acc: 0.9158 - val_loss: 4.2811 - val_acc: 0.4363\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.49460\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 333s 2s/step - loss: 0.9830 - acc: 0.9355 - val_loss: 3.9799 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.49460\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 334s 2s/step - loss: 0.9276 - acc: 0.9493 - val_loss: 4.1873 - val_acc: 0.4650\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.49460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f521dcfe860>"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"/content/gdrive/My Drive/MyCNN/64/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "schedule = SGDRScheduler(min_lr=1e-2,\n",
    "                         max_lr=1e-1,\n",
    "                         steps_per_epoch=np.ceil(200),\n",
    "                         lr_decay=0.9,\n",
    "                         cycle_length=4,\n",
    "                         mult_factor=1.5)\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/MyCNN/64/epochs:019-val_acc:0.465.hdf5')\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(tr_ch3_1, epochs=30, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "Vv7nouIqulHU",
    "outputId": "9e4fc245-ff23-4241-d4d5-e1784699b233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 347s 2s/step - loss: 0.9324 - acc: 0.9511 - val_loss: 4.0362 - val_acc: 0.4735\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.49460\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 335s 2s/step - loss: 0.9033 - acc: 0.9580 - val_loss: 3.9596 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.49460\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 334s 2s/step - loss: 0.8826 - acc: 0.9631 - val_loss: 3.9392 - val_acc: 0.4836\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.49460\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 332s 2s/step - loss: 0.8693 - acc: 0.9652 - val_loss: 4.0092 - val_acc: 0.4804\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.49460\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.8624 - acc: 0.9646 - val_loss: 4.0514 - val_acc: 0.4794\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.49460\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.8615 - acc: 0.9632 - val_loss: 4.1992 - val_acc: 0.4678\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.49460\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.8722 - acc: 0.9568 - val_loss: 4.2015 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.49460\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 329s 2s/step - loss: 0.8951 - acc: 0.9455 - val_loss: 4.3847 - val_acc: 0.4475\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.49460\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.9593 - acc: 0.9209 - val_loss: 4.9386 - val_acc: 0.3943\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.49460\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 331s 2s/step - loss: 1.0741 - acc: 0.8838 - val_loss: 5.0306 - val_acc: 0.3886\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.49460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f521dcfe0b8>"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(tr_me, epochs=10, steps_per_epoch=200, validation_steps=200,\n",
    "                    callbacks=[checkpoint,schedule],\n",
    "                    shuffle=True,\n",
    "                    validation_data=validation_generator_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4590
    },
    "colab_type": "code",
    "id": "6GRxaML5VVBt",
    "outputId": "94b08f96-e068-4727-8117-9dca73356608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_133 (Separable (None, 32, 32, 64)   403         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 32, 32, 64)   256         separable_conv2d_133[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 32, 32, 64)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_134 (Separable (None, 32, 32, 64)   4736        activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 64)   256         separable_conv2d_134[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 32, 32, 64)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_135 (Separable (None, 32, 32, 64)   4736        activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 32, 32, 64)   0           activation_137[0][0]             \n",
      "                                                                 separable_conv2d_135[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 32, 32, 64)   256         add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 32, 32, 64)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_136 (Separable (None, 32, 32, 64)   4736        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 32, 32, 64)   256         separable_conv2d_136[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 32, 32, 64)   0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_137 (Separable (None, 32, 32, 64)   4736        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 32, 32, 64)   0           add_65[0][0]                     \n",
      "                                                                 separable_conv2d_137[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 32, 32, 64)   256         add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 32, 32, 64)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_138 (Separable (None, 32, 32, 64)   4736        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 32, 32, 64)   256         separable_conv2d_138[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 32, 32, 64)   0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_139 (Separable (None, 32, 32, 64)   4736        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 32, 32, 64)   0           add_66[0][0]                     \n",
      "                                                                 separable_conv2d_139[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 32, 32, 64)   256         add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 32, 32, 64)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_140 (Separable (None, 16, 16, 128)  8896        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 128)  512         separable_conv2d_140[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 128)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_141 (Separable (None, 16, 16, 128)  17664       activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 16, 16, 128)  0           conv2d_17[0][0]                  \n",
      "                                                                 separable_conv2d_141[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 128)  512         add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 128)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_142 (Separable (None, 16, 16, 128)  17664       activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 128)  512         separable_conv2d_142[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 128)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_143 (Separable (None, 16, 16, 128)  17664       activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 16, 16, 128)  0           add_68[0][0]                     \n",
      "                                                                 separable_conv2d_143[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_144 (Separable (None, 16, 16, 128)  17664       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 128)  512         separable_conv2d_144[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 128)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_145 (Separable (None, 16, 16, 128)  17664       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 16, 16, 128)  0           add_69[0][0]                     \n",
      "                                                                 separable_conv2d_145[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 16, 16, 128)  512         add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 128)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_146 (Separable (None, 16, 16, 128)  17664       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 16, 16, 128)  512         separable_conv2d_146[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 16, 16, 128)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_147 (Separable (None, 16, 16, 128)  17664       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 16, 16, 128)  0           add_70[0][0]                     \n",
      "                                                                 separable_conv2d_147[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 16, 16, 128)  512         add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 16, 16, 128)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_148 (Separable (None, 8, 8, 256)    34176       activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_148[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 8, 8, 256)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 256)    33024       add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_149 (Separable (None, 8, 8, 256)    68096       activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 8, 8, 256)    0           conv2d_18[0][0]                  \n",
      "                                                                 separable_conv2d_149[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 256)    1024        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 8, 8, 256)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_150 (Separable (None, 8, 8, 256)    68096       activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_150[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 8, 8, 256)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_151 (Separable (None, 8, 8, 256)    68096       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 8, 8, 256)    0           add_72[0][0]                     \n",
      "                                                                 separable_conv2d_151[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 8, 8, 256)    1024        add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 8, 8, 256)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_152 (Separable (None, 8, 8, 256)    68096       activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_152[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 8, 8, 256)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_153 (Separable (None, 8, 8, 256)    68096       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 8, 8, 256)    0           add_73[0][0]                     \n",
      "                                                                 separable_conv2d_153[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 8, 8, 256)    1024        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 8, 8, 256)    0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_154 (Separable (None, 8, 8, 256)    68096       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_154[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 8, 8, 256)    0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_155 (Separable (None, 8, 8, 256)    68096       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 8, 8, 256)    0           add_74[0][0]                     \n",
      "                                                                 separable_conv2d_155[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 8, 8, 256)    1024        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 8, 8, 256)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_156 (Separable (None, 8, 8, 256)    68096       activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_156[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 8, 8, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_157 (Separable (None, 8, 8, 256)    68096       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 8, 8, 256)    0           add_75[0][0]                     \n",
      "                                                                 separable_conv2d_157[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 8, 8, 256)    1024        add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 8, 8, 256)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_158 (Separable (None, 8, 8, 256)    68096       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 8, 8, 256)    1024        separable_conv2d_158[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 8, 8, 256)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_159 (Separable (None, 8, 8, 256)    68096       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 8, 8, 256)    0           add_76[0][0]                     \n",
      "                                                                 separable_conv2d_159[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 8, 8, 256)    1024        add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 8, 8, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_160 (Separable (None, 4, 4, 512)    133888      activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 512)    2048        separable_conv2d_160[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 4, 4, 512)    0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 512)    131584      add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_161 (Separable (None, 4, 4, 512)    267264      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 4, 4, 512)    0           conv2d_19[0][0]                  \n",
      "                                                                 separable_conv2d_161[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 512)    2048        add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 4, 4, 512)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_162 (Separable (None, 4, 4, 512)    267264      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 4, 4, 512)    2048        separable_conv2d_162[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 4, 4, 512)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_163 (Separable (None, 4, 4, 512)    267264      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 4, 4, 512)    0           add_78[0][0]                     \n",
      "                                                                 separable_conv2d_163[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 4, 4, 512)    2048        add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 4, 4, 512)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_164 (Separable (None, 4, 4, 512)    267264      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 4, 4, 512)    2048        separable_conv2d_164[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 4, 4, 512)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_165 (Separable (None, 4, 4, 512)    267264      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 4, 4, 512)    0           add_79[0][0]                     \n",
      "                                                                 separable_conv2d_165[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 4, 4, 512)    2048        add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 4, 4, 512)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 512)    0           activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 1, 1, 200)    102600      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 200)          0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 200)          0           flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,720,795\n",
      "Trainable params: 2,705,563\n",
      "Non-trainable params: 15,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqEZJDVwg2Tj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7WQcjj2E6FvZ",
    "outputId": "cb4d8e9f-9cf6-433d-cdcc-7859cb2ce583"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.556350202560425, 0.49459999725222586]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD( lr=0.01, decay=1e-6, momentum=0.9,nesterov=True) #\n",
    "\n",
    "\n",
    "model = ResnetBuilder.build_resnet_34((64,64,3),200)\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/MyCNN/64/epochs:004-val_acc:0.495.hdf5')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,#'Adam(0.1)',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.evaluate_generator(validation_generator_64,200)\n",
    "# model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PYZL-YKs-UNZ",
    "outputId": "39f4f72f-9790-436f-8474-deec30fe5992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.4951899968087673\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(validation_generator_64,200) #1514 testing images\n",
    "print(\"Accuracy = \", scores[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_4",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
